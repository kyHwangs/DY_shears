#!/usr/bin/env python
"""
Commmand to launch baobab ntuple production
"""
import argparse
import sys
import re
import glob
import os.path

try:
    from CRABAPI.RawCommand import crabCommand
except ImportError:
    sys.stderr.write("\nCRAB 3 environment needs to be set before running this script and before setting the CMSSW environment (cmsenv or scram runtime).\n\n")
    sys.exit(1)

from httplib import HTTPException
from WMCore.Configuration import Configuration

input_json_file="/afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions15/13TeV/Cert_*-*_13TeV_PromptReco_Collisions15_JSON.txt"

parser = argparse.ArgumentParser(description='Tool from the Shears framwork [1] to launch a Baobab nutple production on the GRID.', epilog='[1]  https://twiki.cern.ch/twiki/bin/viewauth/CMS/ShearsAnalysisFramework')

parser.add_argument('dataset_catalogs', nargs='+',
                    help='Catalog files containing the list of CMS datasets to process')

parser.add_argument('--max-events', type=int, action='store', default=-1,
                    help='Specify the maximum number of events to process for each dataset. The default is to process to full dataset.')

parser.add_argument('--events-per-job', type=int, action='store', default = 100000,
                    help='Specify the number of events each job should process')

parser.add_argument('--no-submit', action='store_true',
                    help='With this option the crab configuration files are created but the crab tasks are not submitted')


args = parser.parse_args()

def getLumiMaskJsonFile():
    """Gets the input json file indicating the certified luminosity section which can be processed."""
    candidates = glob.glob(input_json_file)
    latest_ts = 0
    latest_file = None
    for f in candidates:
        ts = os.path.getmtime(f)
        if ts > latest_ts:
            latest_file = f
            latest_ts = ts
    if not latest_file:
        raise RuntimeError("JSON file required to process real data was not found. We have looked for files named %s" % input_json_file)
    return latest_file


def checkDataSetName(dataset):
    a = dataset.split("/")
    if len(a) != 4:
        sys.stderr.write("Dataset name must contains three part /primary_dataset/conditions/tier")
        return False
    p = re.compile("MINIAOD(|SIM)")
    if not p.match(a[3]):
        sys.stderr.write("Only MINIAOD and MINIOADSIM data tiers are supported.")
        return False
    return True

def getCrabConfig(dataset, unitsPerJob, outDir, maxEvents):
    """Build the crab configuration file."""
    
    dataset_tier = dataset.split("/")[3]

    if dataset_tier.find("SIM") > 0:
        isMC = True
    else:
        isMC = False
    #endif
    
    config = Configuration()

    config.section_('General')
#----------------------------------------------------------------------

#Request name. Used to identify job batch and to name the result directory    
    config.General.requestName = dataset.split('/')[1]

#Switch for the copy of the results to the storage site.
    config.General.transferOutputs = True

#Whether or not to copy the job log files to the storage site.
    config.General.transferLogs = True

    config.section_('JobType')
#----------------------------------------------------------------------

# CMSS Configuration file
    config.JobType.psetName = 'grow_baobabs_cfg.py'
#    config.JobType.psetName = 'pset.py'

    config.section_('Data')
#----------------------------------------------------------------------
# Name of the dataset to analyze
    config.Data.inputDataset = dataset

# List of certified luminosity section, which can be processed
# (applies to real data only)
    if not isMC:
        config.Data.lumiMask = getLumiMaskJsonFile()

# Output directory on the storage site (storage site defined in the Site section)
    config.Data.outLFNDirBase = outDir

#Mode to use to split the task in jobs. 
# for 'Analysis' job type: 'FileBased', 'LumiBased' (recommended for real data), or 'EventAwareLumiBased'
# for 'PrivateMC' job type: 'EventBased'
    config.Data.splitting = 'EventAwareLumiBased'

# Number of units to produce or process
    if maxEvents and maxEvents > 0:
        config.Data.totalUnits = maxEvents
        if maxEvents < unitsPerJob:
            unitsPerJob = maxEvents

# Number of units (events in case of PrivateMC)  per job
    config.Data.unitsPerJob = unitsPerJob

# Switch for publication of output data to DBS
    config.Data.publication = False

    config.section_('User')
#----------------------------------------------------------------------
    
    config.section_('Site')
#----------------------------------------------------------------------

#A list of sites where the jobs should not run.
    config.Site.blacklist = ['T3_US_UMD', 'T3_TW_NTU_HEP', 'T3_GR_Demokritos', 'T3_GR_IASA', 'T2_GR_Ioannina', 'T3_MX_Cinvestav', 'T3_IT_Napoli', 'T2_DE_RWTH', 'T2_UK_SGrid_RALPP', 'T3_RU_FIAN', 'T2_FI_HIP']
    
#Site where the output should be copied. 
# See https://cmsweb.cern.ch/sitedb/prod/sites/ for the list of site names
    config.Site.storageSite = 'T2_CH_CERN'
    
    return config

def submitJobs(dataset, unitsPerJob, outDir, maxEvents):
    """Submit crab jobs to produce the Baobab ntuple from a provided dataset."""
    crab_config = getCrabConfig(dataset, unitsPerJob, outDir, maxEvents)

    crab_cfg_file = "crab_" + dataset.split("/")[1] + ".py"
    f = open(crab_cfg_file, "w")
    f.write(str(crab_config))
    f.close()

    if args.no_submit:
        print "Crab configuration file %s created." % crab_cfg_file
        return

    try:
        res = crabCommand('submit', config = crab_config)
        print res
    except HTTPException, e:
        print e.headers

def main():
    outDirLine = re.compile(r'#\s*output directory[:\s=]+\s*([^\s]+)')
    commentLine = re.compile(r'^\s*^#')
    for dataset_catalog in args.dataset_catalogs:
        try:
            f = open(dataset_catalog)
        except IOError, e:
            print e
            continue
        outDir = None
        for dataset in f:
            m = outDirLine.match(dataset)
            if m and len(m.groups()) > 0:
                outDir = m.groups()[0]
            elif not outDir:
                sys.stderr.write('''Fatal error while processing dataset catalog '%s'.
Output directory line is missing from the input dataset catalog.
The line should be before any dataset name and should have the following format:
# output directory: /store/group/phys_smp/AnalysisFramework/Baobab/.../ntuples.
''' % dataset_catalog)
                break
            #endif not outDir
            dataset=dataset.strip()
            if len(dataset) == 0 or commentLine.match(dataset):
                continue
            if checkDataSetName(dataset):
                submitJobs(dataset, args.events_per_job, outDir, args.max_events)


if __name__ == '__main__':
    main()

